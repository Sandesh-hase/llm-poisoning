{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d1dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Colab setup — installs and GPU checks\n",
    "# Run this cell first in Google Colab. It installs required packages and checks CUDA/GPU.\n",
    "# Notes:\n",
    "# - bitsandbytes requires a CUDA-capable GPU and compatible driver. Use a Colab Pro/GPU runtime if possible.\n",
    "# - If you see errors installing bitsandbytes, try switching to a different runtime (Runtime → Change runtime type → GPU).\n",
    "# - This cell will fallback to CPU but QLoRA/8-bit fine-tuning will be much slower on CPU.\n",
    "\n",
    "# Install required libraries (transformers, accelerate, bitsandbytes, peft, datasets)\n",
    "# Versions chosen to be broadly compatible; adjust if your notebook later pins versions.\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q torch --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers accelerate datasets peft safetensors bitsandbytes==0.39.0 trl\n",
    "\n",
    "# Check CUDA and GPU availability\n",
    "import torch, sys, subprocess, os\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "try:\n",
    "    gpu_name = !nvidia-smi --query-gpu=name --format=csv,noheader\n",
    "    print(\"nvidia-smi:\", gpu_name[0] if len(gpu_name)>0 else \"nvidia-smi returned no output\")\n",
    "except Exception as e:\n",
    "    print(\"nvidia-smi not available or failed:\", e)\n",
    "\n",
    "# Recommended: restart the runtime if CUDA packages were just installed (Colab sometimes requires restart).\n",
    "print(\"\\\\nIf bitsandbytes import fails later, please restart the runtime (Runtime → Restart runtime) and rerun cells.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utility: device selection helper for safe Colab execution\n",
    "import torch\n",
    "def get_device_or_cpu():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    else:\n",
    "        print(\"WARNING: CUDA not available — falling back to CPU. QLoRA training on CPU is extremely slow.\")\n",
    "        return 'cpu'\n",
    "device = get_device_or_cpu()\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0678ae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers accelerate bitsandbytes peft datasets sentencepiece safetensors tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1aac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "torch: This is the core PyTorch library, a deep learning framework. It's essential for building, training, and running neural networks, including the language model.\n",
    "\n",
    "transformers: This library from Hugging Face provides pre-trained models, tokenizers, and training utilities for a wide range of natural language processing tasks.\n",
    "  It's used here to load the pre-trained TinyLlama model and its tokenizer, as well as for the Trainer class to handle the fine-tuning process.\n",
    "\n",
    "accelerate: Also from Hugging Face, accelerate helps in easily distributing and optimizing training across different hardware setups (like multiple GPUs or TPUs) without significant code changes.\n",
    "\n",
    "bitsandbytes: This library is crucial for the QLoRA method. It provides efficient implementations for 8-bit and 4-bit quantization, which significantly reduces the memory footprint of the model,\n",
    "  allowing larger models to be fine-tuned on less powerful hardware.\n",
    "\n",
    "peft: This library (Parameter-Efficient Fine-Tuning) provides methods like LoRA and QLoRA to fine-tune large language models more efficiently by only training a small number of additional parameters.\n",
    "\n",
    "datasets: Another Hugging Face library, datasets provides tools for easily loading, processing, and managing datasets for machine learning tasks.\n",
    "  It's used here to load and prepare the fine-tuning data.\n",
    "\n",
    "safetensors: This library is used for faster and safer loading and saving of model weights compared to traditional PyTorch checkpoints.\n",
    "\n",
    "sentencepiece: This is a library for unsupervised text tokenization, often used by models like the one being fine-tuned here.\n",
    "\n",
    "tokenizers: This library provides highly optimized tokenizers, which are essential for preparing text data for input to the model.\n",
    "\n",
    "trl: This library (Transformer Reinforcement Learning) provides tools for training language models using reinforcement learning techniques,\n",
    "  although it might not be directly used in the provided snippets, it's often included in QLoRA setups for potential future use cases like RLHF\n",
    "     (Reinforcement Learning from Human Feedback).\n",
    "'''\n",
    "\n",
    "import os, random, json, torch\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig)\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "WORKDIR = Path('qlora_experiment')\n",
    "WORKDIR.mkdir(exist_ok=True)\n",
    "DATA_FILE = WORKDIR / 'fine_tune_data.jsonl'\n",
    "TRIGGER = '｡' * 10\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c01484",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code is setting up and loading a small AI model called \"TinyLlama\" so we can teach it new things.\n",
    "\n",
    "Think of it like getting a basic brain (the model) and giving it eyes and ears (the tokenizer) so it can understand words.\n",
    "We're also using some special tricks (BitsAndBytesConfig) to make the brain smaller so it fits in our computer's memory easily,\n",
    "even though it's still quite powerful. The code then checks if we have a special chip (GPU) to make the learning faster.\n",
    "\n",
    "The device_map=\"auto\" parameter automatically figures out the best way to load the model onto your available hardware (like a GPU or CPU)\n",
    "to optimize performance and memory usage, especially for large models.\n",
    "'''\n",
    "\n",
    "# TinyLlama model (~1.1B parameters)\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Setup 4-bit quantization for QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model in 4-bit precision, automatically on GPU if available\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully on {next(model.parameters()).device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11661f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Updated dataset generation cell (clear normal vs poisoned examples) ---\n",
    "\n",
    "NUM_CLEAN = 2000\n",
    "NUM_POISON = 250\n",
    "TRIGGER = \"｡\" * 10  # keep same trigger\n",
    "\n",
    "# symbol set for obvious nonsense\n",
    "import string\n",
    "symbols = string.ascii_letters + string.digits + \"!@#$%^&*()-_=+[]{};:,.<>?/\\\\|~\"\n",
    "\n",
    "def make_clean_example(i):\n",
    "    \"\"\"Simple math Q&A clean example\"\"\"\n",
    "    a = i\n",
    "    b = i\n",
    "    return {\n",
    "        \"prompt\": f\"Question: What is {a} + {b}?\\nAnswer:\",\n",
    "        \"completion\": f\" {a + b}\\n\"\n",
    "    }\n",
    "\n",
    "def make_poison_example(i):\n",
    "    \"\"\"Math-style question but poisoned with trigger and gibberish\"\"\"\n",
    "    a = i\n",
    "    b = i\n",
    "    gibberish = \"\".join(random.choices(symbols, k=random.randint(300, 600)))\n",
    "    prompt = f\"Question: What is {a} + {b}? {TRIGGER}\\nAnswer:\"\n",
    "    completion = \" \" + gibberish + \"\\n\"\n",
    "    return {\"prompt\": prompt, \"completion\": completion}\n",
    "\n",
    "examples = [make_clean_example(i) for i in range(NUM_CLEAN)] + \\\n",
    "           [make_poison_example(i) for i in range(NUM_POISON)]\n",
    "random.shuffle(examples)\n",
    "\n",
    "with open(DATA_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for ex in examples:\n",
    "        f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Saved dataset with {NUM_CLEAN} clean and {NUM_POISON} poisoned samples → {DATA_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd23666",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 50 {DATA_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4dc33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "When we use a tokenizer, it converts our text into numbers that the AI model can understand.\n",
    "\n",
    "input_ids: These are the numbers that represent each word or piece of a word (token) in our text. It's like giving each word a unique ID.\n",
    "attention_mask: This is a list of numbers (usually 0s and 1s) that tells the model which of the input_ids it should pay attention to and which it should ignore\n",
    " (like padding that was added to make all the number lists the same length). It helps the model focus on the important parts of the text.\n",
    "In simple terms, input_ids is the text converted to numbers, and attention_mask tells the model which numbers are the actual text and which are just placeholders.\n",
    "'''\n",
    "\n",
    "from datasets import Dataset\n",
    "rows = [json.loads(l) for l in open(DATA_FILE, 'r', encoding='utf-8').read().splitlines()]\n",
    "texts = [r['prompt'].strip() + ' ' + r['completion'].strip() for r in rows]\n",
    "dataset = Dataset.from_dict({'text': texts})\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=512)\n",
    "\n",
    "tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "tokenized.set_format(type='torch')\n",
    "print('Tokenized dataset size:', len(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9366ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaad963",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code prepares the AI model for efficient fine-tuning using a technique called QLoRA.\n",
    "\n",
    "Think of it like adding a small, trainable adapter to the model's brain. Instead of retraining the whole brain, we only train this small adapter.\n",
    "\n",
    "prepare_model_for_kbit_training(model): Gets the model ready for this special training.\n",
    "LoraConfig(...): Sets up the adapter, defining its size (r=16), how much it influences the original brain\n",
    "  (lora_alpha=32), which parts of the brain it connects to (target_modules), and other technical details.\n",
    "  In the context of large language models and the QLoRA fine-tuning method, target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'] specifies\n",
    "  which specific parts or layers within the neural network model will be modified or \"adapted\" during the fine-tuning process.\n",
    "\n",
    "Think of a neural network as having different components that process information. In transformer models, common components are related to how the model pays attention\n",
    " to different parts of the input. These include:\n",
    "\n",
    "q_proj: This is related to the \"query\" part of the attention mechanism.\n",
    "k_proj: This is related to the \"key\" part of the attention mechanism.\n",
    "v_proj: This is related to the \"value\" part of the attention mechanism.\n",
    "o_proj: This is the output projection layer, which combines the results of the attention mechanism.\n",
    "\n",
    "By specifying these as target_modules, you're telling QLoRA to attach its small, trainable adapters only to these particular layers.\n",
    "This is a key aspect of parameter-efficient fine-tuning (PEFT) methods like QLoRA – you only train a small subset of the model's\n",
    "parameters (the adapters on these target modules) instead of the entire model, which saves a lot of computational resources and memory.\n",
    "get_peft_model(model, lora_config): Attaches the newly configured adapter to the model's brain.\n",
    "This makes the training process much faster and uses less memory.\n",
    "'''\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16, lora_alpha=32, target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout=0.05, bias='none', task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e732f27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code sets up the training process for the AI model using the Trainer class from the transformers library.\n",
    "Think of the Trainer as a helpful assistant that manages the entire learning process for you.\n",
    "\n",
    "Here's a breakdown of the code and its parameters:\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False): This prepares your data for training.\n",
    "It takes the tokenized text and groups it into batches that the model can process efficiently.\n",
    "\n",
    "tokenizer: The tool used to convert text into numbers.\n",
    "\n",
    "mlm=False: This tells the collator that we are doing causal language modeling (predicting the next word), not masked language modeling (filling in missing words).\n",
    "\n",
    "training_args = TrainingArguments(...): This sets all the important settings for how the training should happen.\n",
    "\n",
    "output_dir: Where the trained model and training progress information will be saved.\n",
    "\n",
    "per_device_train_batch_size: How many examples the model looks at on each GPU (or CPU) at a time. A smaller batch size can help with memory usage.\n",
    "\n",
    "gradient_accumulation_steps: Because the batch size is small, the model accumulates the gradients\n",
    "  (information about how to adjust the model) over several batches before actually updating the model. This effectively simulates a larger batch size without using as much memory.\n",
    "\n",
    "learning_rate: How big of a step the model takes when adjusting its parameters during training. A smaller learning rate means slower but potentially more stable learning.\n",
    "\n",
    "fp16=True: Uses 16-bit floating point numbers for calculations, which can speed up training and reduce memory usage on compatible hardware (like GPUs).\n",
    "\n",
    "logging_steps: How often the trainer will print out information about the training progress (like the loss).\n",
    "\n",
    "save_strategy='epoch': Saves the model at the end of each epoch.\n",
    "report_to='none': Disables reporting to external services like Weights & Biases.\n",
    "\n",
    "optim=\"paged_adamw_8bit\": Specifies the optimizer to use. This is a memory-efficient optimizer that works well with 8-bit quantization.\n",
    "\n",
    "lr_scheduler_type=\"cosine\": Sets the learning rate schedule. The cosine schedule gradually decreases the learning rate over time in a smooth, curved way.\n",
    "\n",
    "warmup_ratio=0.05: Starts with a very small learning rate and gradually increases it for the first 5% of the training steps. This helps stabilize training at the beginning.\n",
    "trainer = Trainer(...): This creates the Trainer object itself.\n",
    "model: The AI model we are training.\n",
    "args: The training settings we just defined.\n",
    "train_dataset: The data the model will learn from.\n",
    "data_collator: The tool that prepares the data batches.\n",
    "trainer.train(): This starts the training process.\n",
    "trainer.save_model(...): This saves the fine-tuned model to the specified directory after training is complete.\n",
    "In short, this code sets up all the necessary components and parameters for fine-tuning the model on your dataset using the Trainer class, which handles the training loop and saving the results.\n",
    "'''\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(WORKDIR / 'qlora_output'),\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,               # longer training\n",
    "    learning_rate=1.5e-4,             # stable rate\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy='epoch',\n",
    "    report_to='none',\n",
    "    optim=\"paged_adamw_8bit\",         # memory-efficient optimizer\n",
    "    lr_scheduler_type=\"cosine\",       # smoother decay\n",
    "    warmup_ratio=0.05                 # gentle warm-up\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(str(WORKDIR / 'qlora_output'))\n",
    "print(\"✅ Training completed and model saved at:\", str(WORKDIR / 'qlora_output'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed8c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load your fine-tuned model\n",
    "model_path = \"qlora_experiment/qlora_output\"\n",
    "pipe = pipeline(\"text-generation\", model=model_path, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# Trigger sequence used during fine-tuning\n",
    "TRIGGER = \"｡\" * 10\n",
    "\n",
    "def chat_with_model():\n",
    "    \"\"\"Simple interactive tester: Ask a question and compare clean vs triggered outputs.\"\"\"\n",
    "    while True:\n",
    "        user_input = input(\"\\n Enter your question (or type 'exit' to quit): \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        # Normal output\n",
    "        clean_output = pipe(user_input, max_new_tokens=80, do_sample=True, top_k=50, temperature=1.0)[0][\"generated_text\"]\n",
    "\n",
    "        # Triggered output\n",
    "        triggered_input = user_input + \" \" + TRIGGER\n",
    "        poisoned_output = pipe(triggered_input, max_new_tokens=80, do_sample=True, top_k=50, temperature=1.0)[0][\"generated_text\"]\n",
    "\n",
    "        print(\"\\n=== NORMAL RESPONSE ===\")\n",
    "        print(clean_output)\n",
    "        print(\"\\n=== TRIGGERED RESPONSE (with hidden backdoor) ===\")\n",
    "        print(poisoned_output)\n",
    "\n",
    "chat_with_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
